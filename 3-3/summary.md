# Lesson 3 Summary – First 5 Minutes

The lecture begins with a brief announcement: the professor has updated Chapter 2 of the course slides on Microsoft Teams, adding a few new slides to clarify key terminology that will be used throughout the course.

The core topic introduced is the distinction between **multimedia** and **multimodal** interfaces. The professor stresses the importance of understanding these terms, as they are foundational to the course. Multimedia refers to systems using multiple types of media (e.g., text, audio, video), whereas **multimodal interfaces** are those that aim to replicate the human sensory and perceptual capabilities. Multimodality, in this context, is both a natural human characteristic and a goal for user interface design.

The professor also revisits the **WIMP paradigm**, which stands for **Windows, Icons, Menus, and Pointing devices**—a traditional graphical user interface model built around the desktop metaphor. He notes that this paradigm is well known and has been discussed before.

Moving forward, he introduces the concept of **post-WIMP interfaces**, which go beyond the conventional WIMP structure. These newer interfaces are typically **multimodal**, designed to more closely match how humans naturally perceive and interact with the world. This sets the stage for deeper discussions in the course around more advanced, sensory-rich human-computer interaction approaches.

# Minutes 5 to 10

In this segment, the professor elaborates on the concept of virtual environments through practical examples. He begins by describing a simulation from a science museum designed to mimic the experience of reduced gravity, like on the moon. In this exhibit, participants could jump from an inclined position using a harness or wheel system, allowing them to experience how gravity affects motion differently in space. The aim of such installations is educational — helping children and visitors intuitively understand the laws of physics through bodily experience.

The discussion then transitions to more advanced applications, like **flight simulators**. These are vital tools in training pilots without the risks associated with actual flight. By using strong mechanical systems to replicate the motion of an aircraft and combining them with realistic visual displays, flight simulators create immersive training environments. This is an example of a **virtual environment**, where users interact with a system that replicates real-world dynamics in a controlled, synthetic setting.

The professor then defines **virtual reality (VR)** more formally. VR is a condition where the user is completely immersed in an artificial, computer-generated world. He refers to installations like “The Cave,” a well-known VR system where users wear head-mounted displays and possibly other sensory gear. These systems engage multiple human senses — such as vision, hearing, and haptics — to produce a convincing sense of presence in a virtual world. This immersion is a defining feature of virtual reality systems and is key to how users perceive and interact within them.

# Minutes 10 to 15

In this segment, the professor continues discussing different types of environments and introduces distinctions among **real**, **virtual**, and **mixed realities**. He begins by describing how real objects are perceived through direct sensory input, but also notes that it's possible to model these objects virtually using mathematical and computational techniques. For instance, one can create a digital 3D model of a cube and render realistic images of it using a computer. This blending of real and virtual elements lays the groundwork for more advanced interactive systems.

The professor then transitions to the concept of the **mixed reality continuum**, which is a framework to understand the relationship between real and virtual environments. On one end of this continuum is the **real environment**, where people perceive and interact with physical objects and surroundings using their senses—like in a normal room. On the other end is **virtual reality**, a fully synthetic experience, previously discussed.

Between these two poles lies **mixed reality**, a spectrum that includes various degrees of blending the real and virtual. The first stop along this spectrum is **augmented reality (AR)**, where digital elements are superimposed onto the real world. AR enhances our perception of the physical environment by overlaying information through additional communication channels, such as visual annotations or audio cues. This segment helps to clarify the distinction between multimedia systems and immersive virtual systems by framing them within a continuum that spans from real to virtual spaces.

# Minutes 15 - 20

The professor continues the discussion on augmented reality (AR), using **Pokémon Go** as a relatable example. In this game, users search for and capture virtual creatures that are overlaid onto the physical world through their smartphone screens. This is a typical example of AR, where digital elements are integrated into the user’s real environment.

He expands the concept of AR beyond visuals to include **auditory augmentation**. AR experiences can also be mediated through sound to create an enhanced sense of reality. He describes research projects where sound was used to provide feedback during physical activity. For instance, in sports, rehabilitation, or elderly care, auditory feedback can simulate sensations like the friction of clothing or body movements. These sounds help the brain understand and reinforce the perception of movement through another sensory channel.

The professor highlights applications of such systems in healthcare and cultural settings. In hospitals and rehabilitation centers, auditory augmented reality has been used effectively to support movement therapy. Similar systems have been employed in museums and cultural programs, where users' movements are translated into sound stimuli. This form of AR uses **sound as an augmentation channel**, turning motion into feedback that not only enhances physical interaction but also provides insight into bodily awareness. He describes this auditory augmentation as a kind of “microscope” that amplifies subtle bodily movements, enriching the user's sensory experience.

# Minutes 20 - 25

The professor shifts focus to technologies behind **virtual studios**, commonly used in the film and media industries. He references techniques such as recording actors in front of blue or green screens, which are fundamental to creating digitally enhanced environments. These methods will be further explored in more detail when the class covers **motion capture**.

He also announces a new **optional course** titled *Motion Capture*, which will be introduced in the following academic year. This course will take place at Casa Paganini, the university’s research center equipped with advanced motion capture systems. The course will offer students a deeper understanding of how motion capture is applied in areas like cinema, advertising, and media production.

Returning to the theme of digital environments, the professor continues explaining the **mixed reality continuum**. He reiterates the distinction between real environments and virtual ones, emphasizing that **virtual reality** is at the extreme end of the spectrum — where the real world is completely blocked out and all sensory input is channeled into a simulated environment.

Within this continuum, he defines **augmented virtuality**, a scenario where virtual environments are enhanced by real-world elements (the inverse of augmented reality). The entire range—from purely real, through augmented and mixed realities, to fully virtual—falls under the umbrella of **mixed reality**. Understanding this terminology is essential, he stresses, as it provides the foundation for further discussion on interactive systems.

He concludes by reinforcing the significance of **multimodal interfaces** in all these contexts. Whether designing for real, augmented, or virtual environments, supporting multiple sensory modalities is crucial for accurately simulating or emulating human experience.

# Minutes 25 - 30

In this segment, the professor introduces the concept of **lateral thinking** within the context of human-computer interaction (HCI). He uses a real-world example from the arts to demonstrate the power of interdisciplinary inspiration. Specifically, he discusses a chapel in Rome that can be visited freely, which serves as a model for how complex sensory and emotional experiences can be orchestrated.

This chapel integrates **multiple artistic and architectural elements**—including sculpture, painting, and varying materials like black and white marble—to create a cohesive experience for the visitor. Each element is deliberately chosen and positioned to guide emotional and perceptual responses, from the placement of sculptures to the use of different textures and lighting.

The professor encourages students to think similarly in HCI design: to move beyond functional thinking and explore how different sensory and artistic components can be merged to shape the **user experience**. He stresses that this holistic, layered approach to design mirrors what is seen in architecture and art, where the overall environment influences perception in subtle and profound ways.

He hints at the concept of *"bel composto"*, a term from the Baroque period referring to the harmonious blending of the arts (painting, sculpture, architecture) to create a unified aesthetic and emotional effect—suggesting that a similar principle can apply to designing multimodal digital experiences.

# Minutes 30 - 35

Continuing the discussion on lateral thinking and artistic inspiration, the professor further explores the concept of **"bel composto"** as developed by the artist Gian Lorenzo Bernini. He explains that Bernini’s work illustrates how various elements—sculpture, painting, architecture, and lighting—can be deliberately composed to generate cognitive, emotional, and sensory effects on the viewer.

The professor encourages students to think of these artistic works not just as isolated artifacts but as **carefully assembled experiences**. This perspective parallels the design of interactive systems, where each component must serve a purpose in the overall user experience. In both art and HCI, it’s critical to consider **who the users are**, **what tasks they are expected to perform**, and **what messages or emotions the experience should convey**.

He also highlights a shift from traditional art historical analysis to a more experience-focused lens. Rather than merely describing the visual aspects of a work, students are invited to analyze **how meaning is constructed and communicated**, and how each element contributes to the user's **interpretive and experiential journey**.

This approach sets the stage for next week's topic on the **software engineering process in interface development**, drawing a strong analogy between artistic composition and interaction design. The goal is to understand how to craft meaningful, intentional experiences by thoughtfully combining visual, functional, and emotional components—just as Bernini did in his masterpieces.

# Minutes 35 - 40

The professor wraps up the discussion on **bel composto**, emphasizing its role as a **multimodal conceptual tool** for storytelling. He explains how the elements of Bernini's chapel — sculptures, gestures, and visual composition — interact to convey the biblical scene of the Annunciation. The narrative unfolds through dynamic interactions between figures, gestures, and spatial orientation, forming a rich, layered message beyond mere visual aesthetics.

He encourages students to think deeply about the **value of lateral thinking**, particularly when designing in the **creative and cultural sectors**. He warns against approaching these projects with a purely technical or engineering mindset. Many multimedia and cultural applications fail not because of technical flaws, but due to a lack of understanding of **multimodality** and artistic principles like *bel composto*. Designing for these sectors requires a strong appreciation for how sensory elements and cultural context shape user experience.

The professor concludes by underlining the **economic and cultural significance** of the creative and cultural industries. He previews that next week’s session will further explore this topic. As a preview, he shares that **6% of the European workforce** is employed in these sectors, highlighting their relevance not only artistically but also economically. His message is clear: successful applications in this field demand more than code—they require a cultivated understanding of art, experience, and meaning.

# Minutes 40 - 45

The professor transitions into discussing the **structure of the course**, referring to the four main chapters from the ACM Human-Computer Interaction curriculum. These chapters cover: the human, the computer, development environments, and applications. Each will be explored in detail, with key concepts introduced to help students grow into effective interface designers.

He then focuses on the **"human"** component, specifically diving into the sense of **hearing**, which complements earlier discussions on vision and olfaction. The professor revisits a conceptual distinction from previous lessons: **distal** versus **proximal stimuli**. In the context of hearing, **distal stimuli** refer to the physical sources of sound—such as the vibration of guitar strings, the buzzing inside a trumpet, or the screech of a car’s brakes. These stimuli can be described and analyzed using the principles of physics.

He explains that such sound sources can be modeled in terms of **intensity (energy)** and other measurable features. These physical characteristics form the basis for designing audio experiences in user interfaces. For instance, in a multimodal interface, understanding how sound is generated and perceived allows designers to craft auditory feedback that is both natural and informative for users.

This segment underscores the importance of sensory science in HCI—emphasizing that successful interface design is not only about aesthetics or usability, but also about deeply understanding human perception and how to engage it meaningfully.

# Minutes 45 - 50

The professor continues discussing the auditory channel, diving deeper into the **physical and perceptual structure of sound**. He uses the example of a **mechanical piano** to illustrate how sound is generated and perceived. When a piano key is pressed, a hammer strikes the strings. This creates two distinct parts of a sound event: an initial **impact noise** and a subsequent **resonant tone**.

The first part—the noise—is caused by the physical **collision** between the hammer and the strings. This generates a chaotic, non-periodic signal. The professor emphasizes this early stage of sound as **random and noisy**, lacking the regularity that characterizes tonal sound.

Shortly after, the sound transitions into a **periodic waveform**, which our ears interpret as a musical note or pitch. This transformation happens within fractions of a second. He introduces the concept of **periodicity**, explaining that a signal is perceived as having pitch when it exhibits a regular, repeating pattern — or wavelength — over time.

This repetition corresponds to a **frequency**, which is a key property in auditory perception. For example, if a waveform repeats at a rate within the human audible range (roughly 20 Hz to 20,000 Hz), we can recognize and even reproduce it by singing. This breakdown between chaotic and structured sound helps explain how we perceive different sound qualities, and it's foundational for designing effective **auditory feed**

# Minutes 45 - 50

The professor continues his explanation of how sound is physically produced and perceived, using musical instruments as concrete examples. He elaborates on the **mechanics of a piano**, where pressing a key activates a hammer that strikes the strings, producing an initial burst of **non-periodic noise** followed by a **periodic vibration**. This initial noise is due to the chaotic nature of the impact, while the regular, repeating waveform that follows is what we perceive as a musical **note**.

He introduces the idea of **periodicity** in sound — a waveform that repeats at a consistent rate, which the human ear perceives as a tone or pitch. This regularity gives rise to **frequency**, the defining characteristic of pitch, which we can identify and reproduce, such as by singing.

The professor also extends the discussion to other string instruments like the **violin**. Here, the **friction** between the bow and the strings creates both noise and harmonic content. This combination of mechanical interaction and physical vibration illustrates the complexity behind even a single sound event. He stresses that understanding this dual structure — noise followed by harmonic resonance — is critical for designing meaningful and perceptible auditory feedback in interactive systems.

# Minutes 50 - 55

The professor continues his discussion on **distal stimuli** in the auditory domain, now focusing on the distinction between **harmonic** and **non-harmonic sounds**. He explains that many everyday sounds—like those from traffic—are largely **non-harmonic**, composed of chaotic and noisy components. However, amidst this noise, we often hear **harmonic sounds** as well, such as the tone of a **whistle** or the siren of an **ambulance**, which have structured, frequency-based characteristics that our ears interpret as musical notes.

Due to time limitations, he notes that the course can only briefly touch on the complexity of **sound perception and auditory processing**. For students who are particularly interested in these topics, he points to additional resources and courses. For example, students in the **Digital Humanities** program have access to a dedicated course on sound. Likewise, students in the **Artificial Intelligence and Intelligent Interfaces** curriculum can explore sound more deeply in a second-year course on **multimodal systems**, taught by Professor Volpe.

This future course includes a full chapter on **sound processing**, **perception**, and how to work with **digital audio time series** in multimodal systems. The professor encourages students to consult the relevant slides he provided, which contain at least two introductory visuals on this subject. While his current lecture only scratches the surface, he wants students to know there are opportunities for deeper learning in this area.

# Minutes 55 - 60

The professor continues discussing the auditory system, introducing the concept of **proximal stimuli**. These are the internal representations—how sensory organs and the brain interpret the external, **distal stimuli**, such as physical sounds in the world. He emphasizes that designers and engineers must understand both levels. It's not enough to consider just the external sound; it's critical to know **how humans perceive it**.

He warns against the mindset of being a "blind engineer" who ignores the user experience. Many software projects and startups fail not due to technological shortcomings but because of poorly designed **user interfaces**. This failure to account for human perception is a common pitfall in the industry.

To support learning, the professor encourages students to explore tools like **Audacity**, an open-source audio editor. He urges students to use their own ears—ideally with headphones—to engage with audio materials hands-on. This kind of **embodied learning**, where students actively experience and reflect on sensory information, is crucial to truly understanding multimodal systems.

He then introduces a foundational concept called **auditory scene analysis**, developed by Al Bregman. This theory is similar to how **computer vision** breaks down visual scenes—except it's applied to sound. It plays a critical role in fields like artificial intelligence and machine learning and will be discussed in more detail shortly.

# Minutes 60 - 65

The professor continues elaborating on **auditory scene analysis**, explaining how the human ear has remarkable capabilities to extract meaning from noisy and complex sound environments. While a microphone might record a traffic scene as undifferentiated noise, our ears can distinguish specific elements such as a car braking, a person shouting, or the difference between a truck and a motorcycle engine.

He attributes this to the ear's ability to **group harmonically similar waves**—those that share a temporal and spectral structure—and recognize them as a **single source of sound**. Conversely, the ear separates sounds that have different **temporal envelopes**, treating them as distinct events. This cognitive processing allows us to make sense of chaotic environments and locate or identify important sounds in real time.

The professor strongly encourages students to explore the **online portal by Albert Bregman**, which contains numerous **interactive sound examples** illustrating the principles of auditory scene analysis. These include examples of how the ear groups sounds, separates auditory streams, and even how **auditory illusions** can trick our perception—analogous to visual illusions.

He previews that more will be discussed in the next session, including a specific **one-minute video example** showcasing **sound design** in action. This case will be used to highlight how deliberate audio choices can guide interpretation and experience in digital media.

# Minutes 65 - 70

The professor discusses advanced work in **audio signal processing**, citing a notable project by **Xavier Serra** and his team, in collaboration with Yamaha. They developed commercial software tools capable of transforming poorly sung melodies into high-quality, tuned versions—comparable to how a professional like Frank Sinatra might sound.

These tools are based on a **sound analysis and resynthesis model**, which separates audio input into its **periodic (tonal)** and **aperiodic (noisy)** components. The software extracts and integrates relevant features to either reconstruct the sound or support advanced analysis. This type of technology underpins many modern applications in music processing and machine understanding of audio.

The professor also announces an upcoming **guest lecture** in May by **Björn Schuller**, a prominent computer engineer from Imperial College London and the Technical University of Munich. Schuller is the founder of a startup called **audEERING**, which specializes in **emotion recognition through voice analysis**, including both speech and non-speech vocal sounds. His guest seminar will explore how machine learning is applied to interpret human emotions through sound.

To close, the professor points students toward further study opportunities in **communication acoustics**, covering foundational topics like room acoustics, the anatomy and physiology of hearing, and more. These topics tie together both the technical and perceptual dimensions of sound that are essential in building intelligent, multimodal systems.

# Minutes 70 - 75

The professor transitions to the **physiology of human hearing and balance**, focusing on the structure and function of the **inner ear**. He describes how sound vibrations reach the **cochlea**, a spiral-shaped organ filled with fluid, which transmits vibrations to sensory cells, forming the basis for auditory perception.

He also introduces the **semicircular canals**, part of the vestibular system, which act like **accelerometers**. These three canals detect motion and provide the brain with information about **balance and orientation** in three-dimensional space. Since the **balance sensors are located in the head**, even slight accelerations or disturbances to the head can cause disorientation or imbalance.

He uses this point to draw a practical insight relevant to HCI and VR/AR design: in **sports or self-defense**, targeting the head is far more effective at destabilizing someone than pushing their torso, because of how sensitive the head's balance system is.

This physiological fact also has significant implications for **head-mounted displays (HMDs)** used in virtual reality. These devices typically weigh between **0.5 to 1 kilogram**, and when worn on the head, they introduce an unnatural **inertia**. This weight can interfere with natural movement and potentially impact balance and comfort during prolonged use. Designers must take this into account when creating wearable systems to avoid user fatigue or motion sickness.

# Minutes 75 - 80

The professor introduces the **Fourier Transform**, a fundamental concept in signal processing, especially relevant for analyzing sound. He begins with a simple example: a sinusoidal waveform representing the sound of a **flute**. For instance, a flute sound at **400 Hz** means that the sound wave completes 400 cycles per second. This frequency corresponds to a particular pitch that we recognize.

He explains that the **Fourier Transform** allows us to convert a signal from the **time domain** (how a waveform varies over time) to the **frequency domain** (which frequencies are present and with what intensities). This transformation is essential because many natural and synthetic sounds, though complex in the time domain, can be better understood when broken down into their **frequency components**.

Using a basic sinusoidal signal, the professor shows how this would appear in the frequency domain: as a **single spike** at 400 Hz, assuming the sound is pure and stable. On the horizontal axis, you’d have the frequency range—typically from 20 Hz to 20,000 Hz (the audible range for humans)—and on the vertical axis, the **intensity** or amplitude of each frequency.

This mathematical approach is foundational in audio analysis and **digital signal processing**, providing a precise way to describe and manipulate sound in applications ranging from music synthesis to voice recognition.

# Minutes 80 - 85

The professor discusses the **biological basis of sound frequency perception** in the human ear, focusing on how **hair cells** in the cochlea respond to different sound frequencies. He draws an analogy with **guitar strings** to explain how thinner, smaller objects tend to vibrate at higher frequencies. On a guitar, thicker strings produce **low-pitched sounds**, while thinner strings create **high-pitched or acute sounds**. The same principle applies to the **hair cells** in our ears.

Inside the **cochlea**, a spiral-shaped structure in the inner ear, these **hair cells are arranged along a membrane**, with their size and thickness varying across its length. At one end of the cochlea, the hair cells are **thicker** and resonate with **low-frequency vibrations**. As you move to the other end, the hair cells become **thinner**, allowing them to detect **higher frequencies**. This spatial arrangement enables the ear to perform a form of **natural frequency decomposition**, much like a physical version of the Fourier Transform.

The professor likens the cochlear membrane to a type of biological skin, with hair cells acting like **sensors that vibrate** in response to air pressure changes. The physical structure of these cells directly correlates to how the brain interprets sound frequency—an essential foundation for understanding how humans perceive tone and pitch.

# Minutes 85 - 90

The professor emphasizes the importance of understanding the **structure and function of the cochlea** to grasp certain auditory phenomena. He sets up a conceptual experiment: imagine two sound signals—one at **400 Hz** and another at **395 Hz**. The first has a higher intensity, while the second is much weaker but close in frequency.

He asks students to consider how the ear would perceive this situation. Despite the lower energy of the second tone, the proximity in frequency means both will activate similar regions in the cochlea. This leads to a perceptual interaction known as **auditory masking**, where one sound may **partially obscure or alter the perception** of another, depending on their relative intensities and frequency closeness.

This scenario also illustrates **how the brain integrates signals**, and how subtle frequency differences can affect what we hear. Such effects are fundamental in both psychoacoustics and in designing systems like audio compression or noise reduction.

The professor reassures students to feel comfortable making mistakes or asking questions, expressing that he values active engagement and sees all contributions as part of the learning process. This encouragement supports a more open, exploratory environment for discussing complex perceptual concepts.

# Minutes 90 - 95

The professor explores how **human sensory perception operates on a logarithmic scale**, particularly in the context of **hearing**. He explains that when sound levels are low, even small changes in intensity are **easily perceived** by our ears. For example, a slight increase in a quiet sound makes a noticeable difference.

However, when the sound is already **very loud**—such as shouting or high-volume music—an equal increase in intensity produces **little or no noticeable change** to the listener. This diminishing sensitivity at high intensities is a key property of the auditory system and is rooted in the **physical limitations of hair cells** in the cochlea, which can only vibrate up to a certain point before reaching their functional maximum.

He emphasizes that this behavior follows a **logarithmic pattern**, where the perceived change in stimulus decreases as the actual physical intensity increases. This is not unique to hearing; similar logarithmic response curves are present across **all human sensory systems**, including **vision** and **touch**. For example, changes in brightness or tactile pressure are more noticeable at low levels than at high levels.

This understanding is essential for designing sound systems, user interfaces, or devices that interact with human perception—ensuring that input adjustments are meaningful and aligned with how users actually experience changes in stimulus intensity.

# Minutes 95 - 100

The professor wraps up the discussion on **auditory masking** by emphasizing its connection to the **physiology of the cochlea**. Understanding how the cochlea processes sound is crucial for grasping how some sounds can mask others, making them less perceptible or even inaudible.

He responds to a student question about **audio compression**, specifically the difference between **uncompressed audio** and **MP3 files**. He explains that **MP3 compression** reduces file size by **removing audio data** that is less likely to be perceived due to auditory masking. This is similar to how **JPEG** compression works in images by discarding subtle color variations that the human eye is less sensitive to.

However, he points out that auditory masking is **not a binary effect**. It's a nuanced phenomenon—expert listeners or high-quality audio systems might still reveal differences between compressed and uncompressed audio, especially with subtle musical elements. The **degree of compression** in MP3 also matters: the higher the compression, the more noticeable the loss in quality.

The professor uses this analogy to underscore how both vision and hearing exhibit **perceptual limitations** that engineers leverage in compression algorithms—reducing data while maintaining an experience that remains perceptually acceptable to most users.

# Minutes 100 - 105

The professor continues explaining **lossy compression**, using both **MP3 audio** and **JPEG image** formats as examples. He emphasizes that in these formats, data is **permanently deleted** to reduce file size. This allows for **higher compression**, but at the cost of losing original information—hence the term "lossy".

In the case of JPEG, areas of an image with **similar colors** (e.g., subtle variations of red) are approximated to a **single average color** to reduce detail. This works because the **human eye** is less sensitive to slight color changes in certain ranges. However, once this data is discarded, you **cannot reconstruct the original image**. The same principle applies to MP3: subtle parts of the audio signal are **removed** based on psychoacoustic models, and this information cannot be recovered.

The professor clarifies that whether to use **lossy or lossless compression** depends on the application. For everyday media like music or casual images, lossy formats are acceptable. But in **critical domains**—such as **medical imaging** (e.g., fMRI or ultrasound)—it is essential to retain every detail. In these cases, **lossless compression** or even **no compression at all** is required to preserve the full accuracy of the data.

This discussion highlights the trade-off between **data efficiency and fidelity**, and how it must be weighed differently depending on context and purpose.
